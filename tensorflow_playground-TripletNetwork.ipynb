{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'visualize_embed_tsne' from 'C:\\\\Users\\\\Karun\\\\Desktop\\\\embedding-research\\\\visualize_embed_tsne.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import tensorflow as tf\n",
    "from triplet_dataset import MnistDataset\n",
    "# import triplet_dataset_arrows as triplet_dataset\n",
    "# importlib.reload(MnistDataset)\n",
    "import visualize_embed as visualize_embed\n",
    "import visualize_embed_tsne as visualize_embed_tsne\n",
    "importlib.reload(visualize_embed)\n",
    "importlib.reload(visualize_embed_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generic functions to initialize convolutional/pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.get_variable(\"weights\", dtype=tf.float32, initializer=initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable(\"biases\", dtype=tf.float32, initializer=initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def compute_euclidean_distances(x, y):\n",
    "    d = tf.square(tf.subtract(x, y))\n",
    "    d = tf.sqrt(tf.reduce_sum(d))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define the triplet network architecture in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Triplet:\n",
    "    \n",
    "    # Create model\n",
    "    def __init__(self):\n",
    "        # Input and label placeholders\n",
    "        with tf.variable_scope('input'):\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='x')\n",
    "            self.xp = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='xp')\n",
    "            self.xn = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='xn')\n",
    "        \n",
    "        with tf.variable_scope('embedding') as scope:\n",
    "            self.o = self.embedding_network(self.x)\n",
    "            scope.reuse_variables()\n",
    "            self.op = self.embedding_network(self.xp)\n",
    "            self.on = self.embedding_network(self.xn)\n",
    "        \n",
    "        with tf.variable_scope('distances'):\n",
    "            self.dp = compute_euclidean_distances(self.o, self.op)\n",
    "            self.dn = compute_euclidean_distances(self.o, self.on)\n",
    "            self.logits = tf.nn.softmax([self.dp, self.dn], name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.pow(self.logits[0], 2))\n",
    "    \n",
    "    def embedding_network(self, x):\n",
    "        dim = 1\n",
    "        with tf.variable_scope('conv1'):\n",
    "            out = 32\n",
    "            w = weight_variable([5, 5, dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = max_pool_2x2(tf.nn.relu(conv2d(x, w) + b))\n",
    "            dim = out\n",
    "            x = h\n",
    "        with tf.variable_scope('conv2'):\n",
    "            out = 64\n",
    "            w = weight_variable([3, 3, dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = max_pool_2x2(tf.nn.relu(conv2d(x, w) + b))\n",
    "            dim = out\n",
    "            x = h\n",
    "        with tf.variable_scope('conv3'):\n",
    "            out = 128\n",
    "            w = weight_variable([3, 3, dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = max_pool_2x2(tf.nn.relu(conv2d(x, w) + b))\n",
    "            dim = out\n",
    "            x = h\n",
    "        with tf.variable_scope('readout'):\n",
    "            gpool = tf.nn.pool(x, [h.get_shape()[1], h.get_shape()[2]], pooling_type=\"MAX\", padding=\"VALID\", name=\"gpool\")\n",
    "            return tf.reshape(gpool, [-1, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the network for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet = Triplet()\n",
    "train_step = tf.train.AdamOptimizer().minimize(triplet.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training loss 0.250129\n",
      "step 5, training loss 0.247234\n",
      "step 10, training loss 0.249114\n",
      "step 15, training loss 0.222907\n",
      "step 20, training loss 0.146447\n",
      "step 25, training loss 0.0081122\n",
      "step 30, training loss 0.000267658\n",
      "step 35, training loss 0.000109195\n",
      "step 40, training loss 1.43226e-09\n",
      "step 45, training loss 1.02848e-11\n",
      "step 50, training loss 1.97968e-07\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "logging_frequency = 5\n",
    "iterations = 100\n",
    "mnist_dataset = MnistDataset()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iterations):\n",
    "        batch = mnist_dataset.generate_train_data(batch_size)\n",
    "        if i % logging_frequency == 0:\n",
    "            loss = sess.run(triplet.loss, feed_dict={triplet.x: batch[0], triplet.xp: batch[1], triplet.xn: batch[2]}) \n",
    "            print('step %d, training loss %g' % (i, loss))\n",
    "        train_step.run(feed_dict={triplet.x: batch[0], triplet.xp: batch[1], triplet.xn: batch[2]})\n",
    "    test_batch = mnist_dataset.generate_test_data(100)[0]\n",
    "    embed = triplet.o.eval({triplet.x: test_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the embedding using PCA and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embed.visualize(embed, test_batch[:,:,:,0]) # Simple PCA\n",
    "visualize_embed_tsne.visualize(embed, test_batch[:,:,:,0]) # t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
