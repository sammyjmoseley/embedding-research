{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import create_dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generic functions to initialize weight/bias variables, and to define common convolution and pooling operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network architecture + loss function and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_18:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"0/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"1/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"2/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"3/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"4/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"5/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"6/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"7/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"8/FC_18/dropout/mul:0\", shape=(?, 121), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Input/Output placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 121], name='input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='output')\n",
    "\n",
    "num_fc_layers = 10\n",
    "num_nodes = 121\n",
    "\n",
    "layers = [x]\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "prev_layer_size = 121                     \n",
    "# Create FC layers\n",
    "for i in range(0, num_fc_layers):\n",
    "    with tf.name_scope('{}/FC'.format(i)):\n",
    "        W_fc = weight_variable([prev_layer_size, num_nodes])\n",
    "        b_fc = bias_variable([121])\n",
    "        h_fc = tf.nn.relu(tf.matmul(layers[-1], W_fc) + b_fc)\n",
    "        print layers[-1]\n",
    "        h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "        layers.append(h_fc_drop)\n",
    "        prev_layer_size = num_nodes\n",
    "        \n",
    "\n",
    "# Readout\n",
    "with tf.name_scope('5/Readout'):\n",
    "    W_fc2 = weight_variable([num_nodes, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(layers[-1], W_fc2) + b_fc2\n",
    "    norm_y_conv = tf.nn.l2_normalize(y_conv, dim=0)\n",
    "\n",
    "# Define loss\n",
    "l2_loss = tf.reduce_mean(\n",
    "    tf.nn.l2_loss(tf.subtract(norm_y_conv, y_)))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(l2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network per-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training loss 0.503562\n",
      "step 50, training loss 0.502117\n",
      "step 100, training loss 0.503753\n",
      "step 150, training loss 0.503433\n",
      "step 200, training loss 0.502871\n",
      "step 250, training loss 0.493297\n",
      "step 300, training loss 0.492976\n",
      "step 350, training loss 0.496417\n",
      "step 400, training loss 0.497018\n",
      "step 450, training loss 0.454415\n",
      "step 500, training loss 0.453868\n",
      "step 550, training loss 0.399878\n",
      "step 600, training loss 0.407253\n",
      "step 650, training loss 0.394383\n",
      "step 700, training loss 0.379364\n",
      "step 750, training loss 0.377259\n",
      "step 800, training loss 0.376433\n",
      "step 850, training loss 0.376162\n",
      "step 900, training loss 0.376004\n",
      "step 950, training loss 0.375897\n",
      "step 1000, training loss 0.375832\n",
      "step 1050, training loss 0.375787\n",
      "step 1100, training loss 0.375757\n",
      "step 1150, training loss 0.375734\n",
      "step 1200, training loss 0.375718\n",
      "step 1250, training loss 0.375705\n",
      "step 1300, training loss 0.375694\n",
      "step 1350, training loss 0.375686\n",
      "step 1400, training loss 0.375678\n",
      "step 1450, training loss 0.375671\n",
      "step 1500, training loss 0.375666\n",
      "step 1550, training loss 0.375661\n",
      "step 1600, training loss 0.375657\n",
      "step 1650, training loss 0.375653\n",
      "step 1700, training loss 0.375649\n",
      "step 1750, training loss 0.375646\n",
      "step 1800, training loss 0.375644\n",
      "step 1850, training loss 0.375641\n",
      "step 1900, training loss 0.375639\n",
      "step 1950, training loss 0.375637\n"
     ]
    }
   ],
   "source": [
    "batch_size = 120\n",
    "logging_frequency = 50\n",
    "iterations = 2000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(iterations):\n",
    "    batch = create_dataset.generate_data_angles(batch_size)\n",
    "    if i % logging_frequency == 0:\n",
    "      loss = sess.run(l2_loss, feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) \n",
    "      print('step %d, training loss %g' % (i, loss/batch_size))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
