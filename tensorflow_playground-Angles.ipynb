{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import create_dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generic functions to initialize weight/bias variables, and to define common convolution and pooling operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network architecture + loss function and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_9:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"0/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"1/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"2/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"3/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"4/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"5/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"6/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"7/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n",
      "Tensor(\"8/FC_9/dropout/mul:0\", shape=(?, 121), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Input/Output placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 121], name='input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='output')\n",
    "\n",
    "num_fc_layers = 10\n",
    "num_nodes = 121\n",
    "\n",
    "layers = [x]\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "prev_layer_size = 121                     \n",
    "# Create FC layers\n",
    "for i in range(0, num_fc_layers):\n",
    "    with tf.name_scope('{}/FC'.format(i)):\n",
    "        W_fc = weight_variable([prev_layer_size, num_nodes])\n",
    "        b_fc = bias_variable([121])\n",
    "        h_fc = tf.nn.sigmoid(tf.matmul(layers[-1], W_fc) + b_fc)\n",
    "        print layers[-1]\n",
    "        h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "        layers.append(h_fc_drop)\n",
    "        prev_layer_size = num_nodes\n",
    "        \n",
    "\n",
    "# Readout\n",
    "with tf.name_scope('5/Readout'):\n",
    "    W_fc2 = weight_variable([num_nodes, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_conv = tf.matmul(layers[-1], W_fc2) + b_fc2\n",
    "    norm_y_conv = tf.nn.l2_normalize(y_conv, dim=0)\n",
    "\n",
    "# Define loss\n",
    "l2_loss = tf.reduce_mean(\n",
    "    tf.nn.l2_loss(tf.subtract(norm_y_conv, y_)) + 0.01*tf.nn.l2_loss(tf.subtract(norm_y_conv, y_conv)))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(l2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network per-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training loss 0.505392\n",
      "step 20, training loss 0.505249\n",
      "step 40, training loss 0.505247\n",
      "step 60, training loss 0.505247\n",
      "step 80, training loss 0.505247\n",
      "step 100, training loss 0.505247\n",
      "step 120, training loss 0.505247\n",
      "step 140, training loss 0.505247\n",
      "step 160, training loss 0.505246\n",
      "step 180, training loss 0.505246\n",
      "step 200, training loss 0.505242\n",
      "step 220, training loss 0.505224\n",
      "step 240, training loss 0.5031\n",
      "step 260, training loss 0.503391\n",
      "step 280, training loss 0.503429\n",
      "step 300, training loss 0.503418\n",
      "step 320, training loss 0.503402\n",
      "step 340, training loss 0.503385\n",
      "step 360, training loss 0.503369\n",
      "step 380, training loss 0.503354\n",
      "step 400, training loss 0.503339\n",
      "step 420, training loss 0.503324\n",
      "step 440, training loss 0.50331\n",
      "step 460, training loss 0.503297\n",
      "step 480, training loss 0.503283\n",
      "step 500, training loss 0.50327\n",
      "step 520, training loss 0.503257\n",
      "step 540, training loss 0.503243\n",
      "step 560, training loss 0.503229\n",
      "step 580, training loss 0.503214\n",
      "step 600, training loss 0.503198\n",
      "step 620, training loss 0.503179\n",
      "step 640, training loss 0.503153\n",
      "step 660, training loss 0.503101\n",
      "step 680, training loss 0.504237\n",
      "step 700, training loss 0.504403\n",
      "step 720, training loss 0.504407\n",
      "step 740, training loss 0.504378\n",
      "step 760, training loss 0.504347\n",
      "step 780, training loss 0.504319\n"
     ]
    }
   ],
   "source": [
    "batch_size = 120\n",
    "logging_frequency = 20\n",
    "iterations = 800\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(iterations):\n",
    "    batch = create_dataset.generate_data_angles(batch_size)\n",
    "    if i % logging_frequency == 0:\n",
    "      loss = sess.run(l2_loss, feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) \n",
    "      print('step %d, training loss %g' % (i, loss/batch_size))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
