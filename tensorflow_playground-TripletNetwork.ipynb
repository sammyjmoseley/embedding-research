{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tensorflow as tf\n",
    "from triplet_dataset import MnistDatasetSmallRotations as MnistDataset\n",
    "# import triplet_dataset_arrows as triplet_dataset\n",
    "# importlib.reload(MnistDataset)\n",
    "import visualize_embed as visualize_embed\n",
    "import visualize_embed_tsne as visualize_embed_tsne\n",
    "# importlib.reload(visualize_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generic functions to initialize convolutional/pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.get_variable(\"weights\", dtype=tf.float32, initializer=initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable(\"biases\", dtype=tf.float32, initializer=initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def compute_euclidean_distances(x, y, w=None):\n",
    "    d = tf.square(tf.subtract(x, y))\n",
    "    if w is not None:\n",
    "        d = tf.transpose(tf.multiply(tf.transpose(d), w))\n",
    "    d = tf.sqrt(tf.reduce_sum(d))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define the triplet network architecture in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Triplet:\n",
    "    \n",
    "    # Create model\n",
    "    def __init__(self):\n",
    "        # Input and label placeholders\n",
    "        with tf.variable_scope('input'):\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='x')\n",
    "            self.xp = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='xp')\n",
    "            self.xn = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='xn')\n",
    "            self.weights = tf.placeholder(tf.float32, shape=[None], name='weights')\n",
    "            self.y_ = tf.placeholder(tf.float32, shape=[None, 10], name='y_')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        \n",
    "        with tf.variable_scope('embedding') as scope:\n",
    "            self.o = self.embedding_network(self.x)\n",
    "            scope.reuse_variables()\n",
    "            self.op = self.embedding_network(self.xp)\n",
    "            self.on = self.embedding_network(self.xn)\n",
    "        \n",
    "        with tf.variable_scope('distances'):\n",
    "            self.dp = compute_euclidean_distances(self.o, self.op)\n",
    "            self.dn = compute_euclidean_distances(self.o, self.on)\n",
    "            self.logits = tf.nn.softmax([self.dp, self.dn], name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope('embed_loss'):\n",
    "            self.embed_loss = tf.reduce_mean(tf.pow(self.logits[0], 2))\n",
    "            \n",
    "        with tf.variable_scope('classifier'):\n",
    "            self.y = self.classification_network(self.o, self.y_, self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('class_loss'):\n",
    "            self.class_loss = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(self.y), reduction_indices=[1]))\n",
    "    \n",
    "    def embedding_network(self, x):\n",
    "        dim = 1\n",
    "        with tf.variable_scope('conv1'):\n",
    "            out = 32\n",
    "            w = weight_variable([5, 5, dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = max_pool_2x2(tf.nn.relu(conv2d(x, w) + b))\n",
    "            dim = out\n",
    "            x = h\n",
    "        with tf.variable_scope('conv2'):\n",
    "            out = 64\n",
    "            w = weight_variable([3, 3, dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = max_pool_2x2(tf.nn.relu(conv2d(x, w) + b))\n",
    "            dim = out\n",
    "            x = h\n",
    "        with tf.variable_scope('conv3'):\n",
    "            out = 128\n",
    "            w = weight_variable([3, 3, dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = max_pool_2x2(tf.nn.relu(conv2d(x, w) + b))\n",
    "            dim = out\n",
    "            x = h\n",
    "        with tf.variable_scope('readout'):\n",
    "            gpool = tf.nn.pool(x, [h.get_shape()[1], h.get_shape()[2]], pooling_type=\"MAX\", padding=\"VALID\", name=\"gpool\")\n",
    "            return tf.reshape(gpool, [-1, 128])\n",
    "        \n",
    "    def classification_network(self, x, y_, dropout):\n",
    "        dim = 128\n",
    "        with tf.variable_scope('fc1') as scope:\n",
    "            out = 256\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout)\n",
    "            w = weight_variable([dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "            dim = out\n",
    "            x = h\n",
    "        \"\"\"with tf.variable_scope('fc2') as scope:\n",
    "            out = 64\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout)\n",
    "            w = weight_variable([dim, out])\n",
    "            b = bias_variable([out])\n",
    "            h = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "            dim = out\n",
    "            x = h\"\"\"\n",
    "        with tf.variable_scope('fc3') as scope:\n",
    "            out = 10\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout)\n",
    "            w = weight_variable([dim, out])\n",
    "            b = bias_variable([out])\n",
    "            self.y = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            return self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the network for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet = Triplet()\n",
    "embed_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"embedding\")\n",
    "embed_train_step = tf.train.AdamOptimizer().minimize(triplet.embed_loss, var_list=embed_train_vars)\n",
    "class_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"classifier\")\n",
    "class_train_step = tf.train.AdamOptimizer().minimize(triplet.class_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0 (0/50000), training loss 0.24948\n",
      "epoch 0 (160/50000), training loss 0.208888\n",
      "epoch 0 (320/50000), training loss 0.177448\n",
      "epoch 0 (480/50000), training loss 0.00574936\n",
      "epoch 0 (640/50000), training loss 0.00073135\n",
      "epoch 0 (800/50000), training loss 0.000369516\n",
      "epoch 0 (960/50000), training loss 2.7824e-08\n",
      "epoch 0 (1120/50000), training loss 7.16799e-08\n",
      "epoch 0 (1280/50000), training loss 8.14933e-08\n",
      "epoch 0 (1440/50000), training loss 4.18311e-11\n",
      "epoch 0 (1600/50000), training loss 2.73193e-05\n",
      "epoch 0 (1760/50000), training loss 1.03003e-05\n",
      "epoch 0 (1920/50000), training loss 1.05038e-11\n",
      "epoch 0 (2080/50000), training loss 8.1261e-11\n",
      "epoch 0 (2240/50000), training loss 5.32728e-11\n",
      "epoch 0 (2400/50000), training loss 3.34033e-11\n",
      "epoch 0 (2560/50000), training loss 8.35656e-06\n",
      "epoch 0 (2720/50000), training loss 8.25187e-07\n",
      "epoch 0 (2880/50000), training loss 8.48162e-05\n",
      "epoch 0 (3040/50000), training loss 5.3985e-05\n",
      "epoch 0 (3200/50000), training loss 0.000116666\n",
      "epoch 0 (3360/50000), training loss 2.50972e-07\n",
      "epoch 0 (3520/50000), training loss 0.114581\n",
      "epoch 0 (3680/50000), training loss 0.000966983\n",
      "epoch 0 (3840/50000), training loss 3.51513e-05\n",
      "epoch 0 (4000/50000), training loss 4.09061e-07\n",
      "epoch 0 (4160/50000), training loss 1.97122e-06\n",
      "epoch 0 (4320/50000), training loss 8.95055e-06\n",
      "epoch 0 (4480/50000), training loss 1.15274e-06\n",
      "epoch 0 (4640/50000), training loss 4.30806e-06\n",
      "epoch 0 (4800/50000), training loss 1.73141e-06\n",
      "epoch 0 (4960/50000), training loss 9.37924e-05\n",
      "epoch 0 (5120/50000), training loss 4.36223e-05\n",
      "epoch 0 (5280/50000), training loss 7.52493e-10\n",
      "epoch 0 (5440/50000), training loss 7.1487e-09\n",
      "epoch 0 (5600/50000), training loss 2.33835e-09\n",
      "epoch 0 (5760/50000), training loss 1.7554e-09\n",
      "epoch 0 (5920/50000), training loss 5.40771e-07\n",
      "epoch 0 (6080/50000), training loss 2.09152e-09\n",
      "epoch 0 (6240/50000), training loss 4.97993e-07\n",
      "epoch 0 (6400/50000), training loss 1.27598e-08\n",
      "epoch 0 (6560/50000), training loss 2.15036e-08\n",
      "epoch 0 (6720/50000), training loss 7.10699e-09\n",
      "epoch 0 (6880/50000), training loss 1.23389e-08\n",
      "epoch 0 (7040/50000), training loss 4.83567e-07\n",
      "epoch 0 (7200/50000), training loss 1.48098e-07\n",
      "epoch 0 (7360/50000), training loss 5.73557e-07\n",
      "epoch 0 (7520/50000), training loss 2.17321e-07\n",
      "epoch 0 (7680/50000), training loss 2.47869e-06\n",
      "epoch 0 (7840/50000), training loss 5.74176e-09\n",
      "epoch 0 (8000/50000), training loss 6.16123e-08\n",
      "epoch 0 (8160/50000), training loss 4.00643e-10\n",
      "epoch 0 (8320/50000), training loss 1.64605e-08\n",
      "epoch 0 (8480/50000), training loss 7.55872e-10\n",
      "epoch 0 (8640/50000), training loss 7.55937e-08\n",
      "epoch 0 (8800/50000), training loss 2.99265e-08\n",
      "epoch 0 (8960/50000), training loss 5.59008e-08\n",
      "epoch 0 (9120/50000), training loss 4.61972e-11\n",
      "epoch 0 (9280/50000), training loss 1.02852e-08\n",
      "epoch 0 (9440/50000), training loss 1.42429e-05\n",
      "epoch 0 (9600/50000), training loss 1.08131e-06\n",
      "epoch 0 (9760/50000), training loss 3.2932e-09\n",
      "epoch 0 (9920/50000), training loss 4.68403e-08\n",
      "epoch 0 (10080/50000), training loss 2.23895e-08\n",
      "epoch 0 (10240/50000), training loss 5.94361e-10\n",
      "epoch 0 (10400/50000), training loss 1.26348e-08\n",
      "epoch 0 (10560/50000), training loss 1.14166e-09\n",
      "epoch 0 (10720/50000), training loss 1.75787e-06\n",
      "epoch 0 (10880/50000), training loss 1.32795e-07\n",
      "epoch 0 (11040/50000), training loss 2.22402e-06\n",
      "epoch 0 (11200/50000), training loss 3.72556e-09\n",
      "epoch 0 (11360/50000), training loss 5.72e-11\n",
      "epoch 0 (11520/50000), training loss 3.99679e-11\n",
      "epoch 0 (11680/50000), training loss 8.44182e-08\n",
      "epoch 0 (11840/50000), training loss 4.30534e-10\n",
      "epoch 0 (12000/50000), training loss 1.22321e-11\n",
      "epoch 0 (12160/50000), training loss 2.14606e-06\n",
      "epoch 0 (12320/50000), training loss 5.73011e-07\n",
      "epoch 0 (12480/50000), training loss 1.68054e-10\n",
      "epoch 0 (12640/50000), training loss 1.18461e-07\n",
      "epoch 0 (12800/50000), training loss 5.79324e-11\n",
      "epoch 0 (12960/50000), training loss 2.20287e-11\n",
      "epoch 0 (13120/50000), training loss 1.22177e-06\n",
      "epoch 0 (13280/50000), training loss 2.25367e-07\n",
      "epoch 0 (13440/50000), training loss 1.70971e-13\n",
      "epoch 0 (13600/50000), training loss 6.40278e-09\n",
      "epoch 0 (13760/50000), training loss 1.77314e-07\n",
      "epoch 0 (13920/50000), training loss 5.89561e-08\n",
      "epoch 0 (14080/50000), training loss 3.82596e-06\n",
      "epoch 0 (14240/50000), training loss 1.78279e-13\n",
      "epoch 0 (14400/50000), training loss 9.60979e-07\n",
      "epoch 0 (14560/50000), training loss 1.05438e-07\n",
      "epoch 0 (14720/50000), training loss 8.46089e-10\n",
      "epoch 0 (14880/50000), training loss 3.19611e-13\n",
      "epoch 0 (15040/50000), training loss 1.37824e-08\n",
      "epoch 0 (15200/50000), training loss 3.36293e-10\n",
      "epoch 0 (15360/50000), training loss 2.72396e-11\n",
      "epoch 0 (15520/50000), training loss 9.86938e-09\n",
      "epoch 0 (15680/50000), training loss 1.69029e-05\n",
      "epoch 0 (15840/50000), training loss 7.42169e-10\n",
      "epoch 0 (16000/50000), training loss 4.95414e-10\n",
      "epoch 0 (16160/50000), training loss 2.72073e-08\n",
      "epoch 0 (16320/50000), training loss 1.15669e-07\n",
      "epoch 0 (16480/50000), training loss 2.67796e-09\n",
      "epoch 0 (16640/50000), training loss 5.37128e-09\n",
      "epoch 0 (16800/50000), training loss 3.28676e-05\n",
      "epoch 0 (16960/50000), training loss 6.89204e-12\n",
      "epoch 0 (17120/50000), training loss 0.000211452\n",
      "epoch 0 (17280/50000), training loss 4.0112e-07\n",
      "epoch 0 (17440/50000), training loss 2.4064e-06\n",
      "epoch 0 (17600/50000), training loss 2.86852e-10\n",
      "epoch 0 (17760/50000), training loss 1.29191e-09\n",
      "epoch 0 (17920/50000), training loss 2.49901e-09\n",
      "epoch 0 (18080/50000), training loss 3.0599e-07\n",
      "epoch 0 (18240/50000), training loss 4.72423e-07\n",
      "epoch 0 (18400/50000), training loss 3.02818e-05\n",
      "epoch 0 (18560/50000), training loss 2.16913e-09\n",
      "epoch 0 (18720/50000), training loss 3.69145e-06\n",
      "epoch 0 (18880/50000), training loss 1.49131e-06\n",
      "epoch 0 (19040/50000), training loss 1.07688e-07\n",
      "epoch 0 (19200/50000), training loss 6.08904e-10\n",
      "epoch 0 (19360/50000), training loss 1.69433e-07\n",
      "epoch 0 (19520/50000), training loss 2.62358e-10\n",
      "epoch 0 (19680/50000), training loss 6.02509e-11\n",
      "epoch 0 (19840/50000), training loss 1.28781e-12\n",
      "epoch 0 (20000/50000), training loss 9.19148e-08\n",
      "epoch 0 (20160/50000), training loss 1.1232e-09\n",
      "epoch 0 (20320/50000), training loss 8.42364e-12\n",
      "epoch 0 (20480/50000), training loss 1.78579e-09\n",
      "epoch 0 (20640/50000), training loss 1.367e-12\n",
      "epoch 0 (20800/50000), training loss 2.46429e-11\n",
      "epoch 0 (20960/50000), training loss 1.61636e-10\n",
      "epoch 0 (21120/50000), training loss 1.89821e-07\n",
      "epoch 0 (21280/50000), training loss 2.64663e-06\n",
      "epoch 0 (21440/50000), training loss 2.05265e-10\n",
      "epoch 0 (21600/50000), training loss 1.5552e-10\n",
      "epoch 0 (21760/50000), training loss 6.30609e-13\n",
      "epoch 0 (21920/50000), training loss 5.42856e-09\n",
      "epoch 0 (22080/50000), training loss 0.00039174\n",
      "epoch 0 (22240/50000), training loss 8.49921e-11\n",
      "epoch 0 (22400/50000), training loss 4.33585e-12\n"
     ]
    }
   ],
   "source": [
    "embed_batch_size = 32\n",
    "embed_iterations = 2\n",
    "logging_frequency = 5\n",
    "class_batch_size = 32\n",
    "class_eval_batch_size = 100\n",
    "class_iterations = 2\n",
    "mnist_dataset = MnistDataset()\n",
    "eval_full = True\n",
    "kp = 0.75\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(embed_iterations):\n",
    "        for j in range((int)(50000/embed_batch_size)):\n",
    "            batch = mnist_dataset.generate_train_data(embed_batch_size)\n",
    "            if j % logging_frequency == 0:\n",
    "                loss = sess.run(triplet.embed_loss, feed_dict={triplet.x: batch[0], triplet.xp: batch[1], triplet.xn: batch[2], triplet.weights: batch[3]}) \n",
    "                print('epoch %d (%d/50000), training loss %g' % (i, j*embed_batch_size, loss))\n",
    "            embed_train_step.run(feed_dict={triplet.x: batch[0], triplet.xp: batch[1], triplet.xn: batch[2], triplet.weights: batch[3]})\n",
    "        batch = mnist_dataset.generate_train_data(embed_batch_size)\n",
    "        loss = sess.run(triplet.embed_loss, feed_dict={triplet.x: batch[0], triplet.xp: batch[1], triplet.xn: batch[2], triplet.weights: batch[3]}) \n",
    "        print('epoch %d, training loss %g' % (i, loss))\n",
    "    test_batch = mnist_dataset.generate_test_data(100)\n",
    "    embed = triplet.o.eval({triplet.x: test_batch})\n",
    "    visualize_embed.visualize(embed, test_batch[:,:,:,0]) # Simple PCA\n",
    "    visualize_embed_tsne.visualize(embed, test_batch[:,:,:,0]) # t-SNE\n",
    "    for i in range(class_iterations):\n",
    "        for j in range((int)(50000/class_batch_size)):\n",
    "            input_images, correct_predictions = mnist.train.next_batch(class_batch_size)\n",
    "            input_images = input_images.reshape(-1, 28, 28, 1)\n",
    "            if j % logging_frequency == 0:\n",
    "                loss, acc = sess.run([triplet.class_loss, triplet.accuracy], feed_dict={triplet.x: input_images, triplet.y_: correct_predictions, triplet.keep_prob:1.0})\n",
    "                print('epoch %d (%d/50000), training loss %g, training accuracy %g' % (i, j*class_batch_size, loss, acc))\n",
    "            class_train_step.run(feed_dict={triplet.x: input_images, triplet.y_: correct_predictions, triplet.keep_prob:kp})\n",
    "        test_input_images, test_correct_predictions = mnist.test.next_batch(class_eval_batch_size)\n",
    "        test_input_images = test_input_images.reshape(-1, 28, 28, 1)\n",
    "        acc = sess.run(triplet.accuracy, feed_dict={triplet.x:test_input_images, triplet.y_:test_correct_predictions, triplet.keep_prob:kp})\n",
    "        print('epoch %d, test acc: %g', i, acc)\n",
    "    if eval_full:\n",
    "        full_input_images = mnist.test.images.reshape(-1, 28, 28, 1)\n",
    "        full_input_labels = mnist.test.labels\n",
    "        acc = sess.run(triplet.accuracy, feed_dict={triplet.x:full_input_images, triplet.y_:full_input_labels, triplet.keep_prob:1.0})\n",
    "        print ('full test acc: %g', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embed.visualize(embed, test_batch[:,:,:,0]) # Simple PCA\n",
    "visualize_embed_tsne.visualize(embed, test_batch[:,:,:,0]) # t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define a classifier network that takes embeddings as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
